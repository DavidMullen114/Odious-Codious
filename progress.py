# -*- coding: utf-8 -*-
"""PROGRESS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_IwD3rvOPSR-8TBjAPO2aFfw3xsv6MYz
"""

from scipy.optimize import curve_fit
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, BatchNormalization, Flatten, Dropout, Conv2D, ReLU, LeakyReLU, AveragePooling2D
import itertools as it
import time
from scipy import linalg
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
#np.set_printoptions(precision=3, suppress=True, linewidth=140)

#-------------------------------------------------------------------------------
# Functions used to generate dataset - the atomization energy
#-------------------------------------------------------------------------------

# array of atom combinations
def gen_combs(n):
  comb_list = []
  comb_obj = it.combinations(range(n),2)
  for comb in comb_obj:
    comb_list.append(comb)
  return np.array(comb_list)

# morse function with distance x - all other variables =1
def morse(x, q=1, m=1, u=1, v=1):
    return (q * (np.exp(-2*m*(x-u))-2*np.exp(-m*(x-u))) + v)

# function to calculate distance between two points in cartesian coordinates
def distance(r_1, r_2):
  (x_1, y_1) = r_1
  (x_2, y_2) = r_2
  return ((x_2 - x_1)**2 + (y_2 - y_1)**2)**(1/2)

#-------------------------------------------------------------------------------
# calculate the atomization energy
#-------------------------------------------------------------------------------
V = []                        # empty list to append final energies to 
R = []                        # empty list to append atom coordinates to 
N = 400000                    # number of samples to generate
width = 1                     # width of box
N_atoms = 4                   # number of atoms
combs = gen_combs(N_atoms)    # array of atom combinations, shape=(N_atoms,2)

# loop through each randomly generated coordinate
for _ in range(N):
  V_i = [] # dummy list to store energies
  
  # generate atom coordinates
  r= []
  for i in range(N_atoms):
    x_i, y_i = np.random.random(width), np.random.random(width)
    r.append([x_i,y_i])
  R.append(r) # R ==> training dataset X

  # loop through each of the possible atom coordinates and calculate energy
  for c in combs:
    (i,j) = c 
    dist = distance(r[j], r[i])
    V_i.append(morse(dist))
  V.append(np.sum(V_i)) # energy of molecule is the sum of the energies between all atom perms

V = np.array(V)
R = np.array(R)

"""-------------------------------------------------------------------------------
Fully Connected Neural network
  - each sample $x$ is cartesian coordinates list w/ shape=(N,8).
  - each label $y$ is atom energy from morse 
  - split whole dataset into training and test sets
    - training set = dataset that the model learns on
    - test (aka validation) set = dataset for YOU to see how well model performs 
      - doesn't learn from this set at all
      - choose the model with lowest validation error - best at performing on data never seen before
  - standardised samples, mean=0 and std=1
  - normalised labels, y$\in$[0,1]
  - fully connected regression model to predict atom energy
    - 8 input nodes as there are 8 inputted values - (x,y) for 4 atoms
    - non-linear activation function = ReLU (like neurons firing)
    - regularising dropout layers = randomly make neurons dead (output=0)
    - 1 output node as only atom energy is scalar
    - output activation function = sigmoid (scales output to [0,1], like y)
    - loss function is mean-squared-error $(y_{pred}-y_{train})^2$ - NETWORK TRIES TO MINIMISE THIS
  - train model (w/ model.fit()) on samples $x_{train}$ and labels $y_{train}$
  - after each epoch (epoch:=trains on whole training dataset once) evaluate test (aka validation) dataset error
  - monitor validation data and if it doesnt improve for three epochs stop training and revert to model with best validation error.
  - print out the predictions and actual atom energies for 10 random atom configs (check preds before training)
  - plot of epoch vs error
-------------------------------------------------------------------------------
"""

#-------------------------------------------------------------------------------
# split and standard/normalise data
#-------------------------------------------------------------------------------

# entire sample dataset
X = np.copy(R).reshape((-1,8))
y = np.copy(V)

# split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# standardize X data
X_train_std = (X_train - np.mean(X_train))/np.std(X_train)
X_test_std = (X_test - np.mean(X_train))/np.std(X_train)

# normalize y data
y_train_std = (y_train - np.min(y_train))/(np.max(y_train) - np.min(y_train))
y_test_std = (y_test - np.min(y_train))/(np.max(y_train) - np.min(y_train))

#-------------------------------------------------------------------------------
# Fully-Connected Model
#-------------------------------------------------------------------------------

model = Sequential()

model.add(Dense(8, input_dim=8))
model.add(ReLU())
#model.add(LeakyReLU(alpha=0.1))
#model.add(BatchNormalization(momentum=0.8))
#model.add(Dropout(0.1))

model.add(Dense(64))
model.add(ReLU())
#model.add(LeakyReLU(alpha=0.1))
#model.add(BatchNormalization(momentum=0.8))
#model.add(Dropout(0.1))

model.add(Dense(128))
model.add(ReLU())
#model.add(LeakyReLU(alpha=0.1))
#model.add(BatchNormalization(momentum=0.8))
model.add(Dropout(0.1))

model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='mse')

model.summary()

#-------------------------------------------------------------------------------
# Training fully-Connected Model
#-------------------------------------------------------------------------------
monitor = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=3, verbose=0, mode='auto', restore_best_weights=True)
history = model.fit(X_train_std, y_train_std, validation_data=(X_test_std, y_test_std), batch_size=32, epochs=20, callbacks=[monitor])

# test of accuracy of predictions of energy y on 10 samples
x_10 = X_test_std[0:10]
y_10 = y_test_std[0:10]

for i in range(10):
  pred = model.predict((x_10[i]).reshape(1,8))
  print('actual = {} | prediction = {}'.format(y_10[i], pred))

print(x_10.dtype)
pred.dtype

plt.figure()
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.title('model loss')
plt.ylabel('mse')
plt.xlabel('epoch')
plt.legend()
plt.show()

"""-------------------------------------------------------------------------------
Convolutional Neural network
  - each sample $x$ is Coulomb matrix w/ shape=(N,4,4,1).
  - each label $y$ is atom energy from morse. 
  - split whole dataset into training and test sets
    - training set = dataset that the model learns on
    - test (aka validation) set = dataset for YOU to see how well model performs 
      - doesn't learn from this set at all
      - choose the model with lowest validation error - best at performing on data never seen before
  - standardised samples, mean=0 and std=1
  - normalised labels, y$\in$[0,1]
  - convolutional network to predict atom energy
    - network input shape = C_matrix shape
    - each Conv2D layer slides a shape=(3,3) filter/kernel over image and multiplies - VALUES OF KERNEL ARE LEARNED, NOT WEIGHTS
    - padding='same' - puts 0 valued pixels around image so as to output image with same shape after kernel convolution
    - stride is number of pixels kernel jumps when sliding over image
    - non-linear activation function = ReLU (like neurons firing)
    - regularising dropout layers = randomly make neurons dead (output=0)
    - average pooling - takes shape=(2,2) square and outputs only 1 pixel with value of average of four inputs - used instead of stride to downsample dimensions
    - 1 output node as only atom energy is scalar
    - output activation function = sigmoid (scales output to [0,1], like y)
    - loss function is mean-squared-error $(y_{pred}-y_{train})^2$ - NETWORK TRIES TO MINIMISE THIS
  - train model (w/ model.fit()) on samples $x_{train}$ and labels $y_{train}$
  - after each epoch (epoch:=trains on whole training dataset once) evaluate test (aka validation) dataset error
  - monitor validation data and if it doesnt improve for three epochs stop training and revert to model with best validation error.
  - plot of epoch vs error
-----------------------------
"""

#-------------------------------------------------------------------------------
# Generating Coulomb matrices 
#-------------------------------------------------------------------------------

Z = np.ones(shape=(N,N_atoms)) # charge of each atom --> all atoms are Hydrogen
C = [] # empty list of matrices

#loop through each coordinate configuration
for n in range(N):
  # Empty list to append to C --> c = C_matrix for each configuration
  c = np.zeros(shape=(N_atoms,N_atoms))
  #loop through each atom's interaction with every other atom
  for i in range(0, N_atoms):
    for j in range(0, N_atoms):
      # matrix diagonals are self-interactions
      if i == j:
        c[i,j] = 0.5*(Z[n,i]**2.4)
      # interaction of i-th atom with j-th atom
      else:
        c[i,j] = (Z[n,i]*Z[n,j])/(np.sqrt(np.sum(np.square(R[n,i]-R[n,j]))))
  C.append(c)

C = np.array(C).reshape(N,N_atoms,N_atoms,1) 
print(C.shape)

#-------------------------------------------------------------------------------
# split and standard/normalise data
#-------------------------------------------------------------------------------

# entire sample dataset
X = np.copy(C)
y = np.copy(V)

# split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# standardize X data
X_train_std = (X_train - np.mean(X_train))/np.std(X_train)
X_test_std = (X_test - np.mean(X_train))/np.std(X_train)

# normalize y data
y_train_std = (y_train - np.min(y_train))/(np.max(y_train) - np.min(y_train))
y_test_std = (y_test - np.min(y_train))/(np.max(y_train) - np.min(y_train))

#-------------------------------------------------------------------------------
# Creating convolutional Model
#-------------------------------------------------------------------------------

model = Sequential()

model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', input_shape=(4,4,1)))
#model.add(ReLU())
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization(momentum=0.8))
model.add(Dropout(0.1))
model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))

model.add(Conv2D(filters=128, kernel_size=(2,2), padding='same'))
#model.add(ReLU())
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization(momentum=0.8))
model.add(Dropout(0.1))
model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))

model.add(Conv2D(filters=512, kernel_size=(2,2), padding='same'))
#model.add(ReLU())
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization(momentum=0.8))
model.add(Dropout(0.1))
#model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))

model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='mse')

model.summary()

#-------------------------------------------------------------------------------
# Training convolutional Model
#-------------------------------------------------------------------------------
monitor = EarlyStopping(monitor='val_loss', min_delta=1e-6, patience=3, verbose=0, mode='auto', restore_best_weights=True)
history = model.fit(X_train_std, y_train_std, validation_data=(X_test_std, y_test_std), batch_size=32, epochs=20, callbacks=[monitor])

# test of accuracy of predictions of energy y on 10 samples
x_10 = X_test_std[0:10]
y_10 = y_test_std[0:10]

for i in range(10):
  pred = model.predict((x_10[i]).reshape(1,4,4,1))
  print('actual = {} | prediction = {}'.format(y_10[i], pred))

plt.figure()
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.title('model loss')
plt.ylabel('mse')
plt.xlabel('epoch')
plt.legend()
plt.show()

"""-------------------------------------------------------------------------------
Convolutional Neural network with potential distributions
  - each sample $x$ is Coulomb matrix w/ shape=(N,4,4,1).
  - each label $y$ is atom energy from morse. 
  - split whole dataset into training and test sets
    - training set = dataset that the model learns on
    - test (aka validation) set = dataset for YOU to see how well model performs 
      - doesn't learn from this set at all
      - choose the model with lowest validation error - best at performing on data never seen before
  - standardised samples, mean=0 and std=1
  - normalised labels, y$\in$[0,1]
  - convolutional network to predict atom energy
    - network input shape = C_matrix shape
    - each Conv2D layer slides a shape=(3,3) filter/kernel over image and multiplies - VALUES OF KERNEL ARE LEARNED, NOT WEIGHTS
    - padding='same' - puts 0 valued pixels around image so as to output image with same shape after kernel convolution
    - stride is number of pixels kernel jumps when sliding over image
    - non-linear activation function = ReLU (like neurons firing)
    - regularising dropout layers = randomly make neurons dead (output=0)
    - average pooling - takes shape=(2,2) square and outputs only 1 pixel with value of average of four inputs - used instead of stride to downsample dimensions
    - 1 output node as only atom energy is scalar
    - output activation function = sigmoid (scales output to [0,1], like y)
    - loss function is mean-squared-error $(y_{pred}-y_{train})^2$ - NETWORK TRIES TO MINIMISE THIS
  - train model (w/ model.fit()) on samples $x_{train}$ and labels $y_{train}$
  - after each epoch (epoch:=trains on whole training dataset once) evaluate test (aka validation) dataset error
  - monitor validation data and if it doesnt improve for three epochs stop training and revert to model with best validation error.
  - plot of epoch vs error
-----------------------------
"""

#------------------------------------------------------------------------------
# Defining variables and matrices
#------------------------------------------------------------------------------

# Define matrix size N
N_pot = 32
n = N_pot ** 2

#------------------------------------------------------------------------------
# FUNCTIONS 
#------------------------------------------------------------------------------    
def plotter_3D(u_1D, title=''):
    '''
    Function that returns a 3D surface plot for the inputted 1D column vector
    with a given title - used to plot potential of point charge
    '''
    u_1D = np.array(u_1D)
    u_grid = u_1D.reshape((N_pot,N_pot))
    
    fig = plt.figure()
    ax = fig.gca(projection='3d')
    x = np.arange(0,N_pot,1)
    y = np.arange(0,N_pot,1)
    x, y = np.meshgrid(x, y)
    ax.plot_surface(x, y, u_grid)
    plt.title(title)
    plt.show()
     
def soln_error(x1, x2):
    return np.sum(np.square(x1-x2))

def A_matrix(N):
    n = N**2
    A = np.zeros(shape=(n,n))

    for i in range(n):
        for j in range(n):
            if i >= N and j >= N and i < n-N and j < n-N:
                if i == j:
                    A[i,j] = 4
                    A[i+N,j] = -1
                    A[i-N,j] = -1
                if i == j+1 or i+1 == j:
                    A[i,j] = -1
     
    for i in range(n):
        for j in range(n):
            
            if i < N:
                A[i,j] = 0
                if i == j:
                    A[i,j] = 1
                    
            if i >= n-N:
                A[i,j] = 0
                if i == j:
                    A[i,j] = 1
                    
            if i >= N and i < n-N:
                if i%N == 0 or (i+(N+1))%N == 0:
                    if i == j:
                        A[:,j] = 0
                        A[i,:] = 0
                        A[i,j] = 1
    return A
A = A_matrix(N_pot)

#------------------------------------------------------------------------------
# SOR METHOD
#------------------------------------------------------------------------------
start_time = time.time()

# Defining maximum iterations and empty list to append pot dists to 
max_iter = 10000
max_error = 1e-6
pot_dists = []
X = []
y = []

# Setting value of w - optimum value depends on N
w = 1.8

# Split A into component parts such that A = L+U+D
A = A_matrix(N_pot)
L = np.tril(A,k=-1)
U = np.triu(A,k=1)
D = np.diagflat(np.diag(A))

M = linalg.inv(D+w*L)
T_og = np.matmul(M, (1-w)*D-w*U)
u_og = np.zeros(shape=(n,))

# loop through samples and if charge is on edge remove from dataset
for i in range(N):
  r = R[i].reshape(8,)
  coords = (r*N_pot).astype(int)
  if 0 in coords or 31 in coords:
    pass
  else:
    X.append(R[i])
    y.append(V[i])

for i in range(len(X)):
  # randomise atom locations
  f = np.zeros(shape=(N_pot,N_pot))
  r = X[i]
  for atom in range(N_atoms):
    (a,b) = r[atom]
    l, k = (a*N_pot).astype(int), (b*N_pot).astype(int)
    f[l,k] = 1
  f = f.reshape((n,))

  # Initializing matrices
  T = np.copy(T_og)
  c = np.matmul(M, w*f)
  u = np.copy(u_og)

  # Update u_new = T*u + c while error<10^-6
  for iter in range(1,max_iter): 
    u_new = np.matmul(T, u) + c
    
    # Calculating error and appending to list
    error = soln_error(u_new, u)
    if error <= max_error:
        break
    u = u_new
  pot_dists.append(u.reshape((N_pot,N_pot)))

pot_dists = np.array(pot_dists) 
y = np.array(y) 

# Defining number of iterations taken and making 3D plot    
plotter_3D(u, 'SOR method (i={})'.format(iter))
print(pot_dists.shape)
print(time.time()-start_time)

np.savez_compressed('pot_dists_{}k.npz'.format(str(pot_dists.shape[0])[:3]), distributions=pot_dists, energies=y)

#mounting google drive
from google.colab import drive
drive.mount('/gdrive')

from google.colab import drive
drive.mount('/content/drive')

#-------------------------------------------------------------------------------
# Loading files to drive
#-------------------------------------------------------------------------------

# from specified path import data
path = '/content/drive/My Drive/David/pot_dists_357k.npz'
with np.load(path) as data:
  X = data['distributions']
  y = data['energies']

#pot_dists = np.concatenate((A,pot_dists))
#y  = np.concatenate((B,y))

#-------------------------------------------------------------------------------
# Saving files to drive
#-------------------------------------------------------------------------------

# Import PyDrive and associated libraries.
# This only needs to be done once in a notebook.
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
# This only needs to be done once in a notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Create & upload a text file.
file_path = '/content/pot_dists_357k.npz'
uploaded = drive.CreateFile({'title':'pot_dists_357k.npz'})
uploaded.SetContentFile(file_path)
uploaded.Upload()
print('Uploaded file with ID {}'.format(uploaded.get('id')))

#-------------------------------------------------------------------------------
# split and standard/normalise data
#-------------------------------------------------------------------------------

# entire sample dataset
X = X.reshape(-1,N_pot,N_pot,1)

# split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# standardize X data
X_train_std = (X_train - np.mean(X_train))/np.std(X_train)
X_test_std = (X_test - np.mean(X_train))/np.std(X_train)

# normalize y data
y_train_std = (y_train - np.min(y_train))/(np.max(y_train) - np.min(y_train))
y_test_std = (y_test - np.min(y_train))/(np.max(y_train) - np.min(y_train))

#-------------------------------------------------------------------------------
# Creating convolutional Model
#-------------------------------------------------------------------------------
from keras import backend as K 
K.set_floatx('float64')

model = Sequential()

model.add(Conv2D(filters=32, kernel_size=(5,5), strides=(2,2), padding='same', input_shape=(N_pot,N_pot,1)))
model.add(LeakyReLU(alpha=0.1))
model.add(Dropout(0.1))
#model.add(BatchNormalization(momentum=0.8))

model.add(Conv2D(filters=64, kernel_size=(5,5), strides=(2,2), padding='same'))
model.add(LeakyReLU(alpha=0.1))
model.add(Dropout(0.1))
#model.add(BatchNormalization(momentum=0.8))

model.add(Conv2D(filters=128, kernel_size=(5,5), strides=(2,2), padding='same'))
model.add(LeakyReLU(alpha=0.1))
model.add(Dropout(0.1))
#model.add(BatchNormalization(momentum=0.8))

model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(2,2), padding='same'))
model.add(LeakyReLU(alpha=0.1))
model.add(Dropout(0.1))
#model.add(BatchNormalization(momentum=0.8))

#model.add(Conv2D(filters=256, kernel_size=(5,5), padding='same'))
#model.add(LeakyReLU(alpha=0.1))
#model.add(Dropout(0.1))
#model.add(BatchNormalization(momentum=0.8))

model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
adam = Adam(lr=0.0002, beta_1=0.5)
model.compile(optimizer=adam, loss='mse')

model.summary()

#-------------------------------------------------------------------------------
# Training convolutional Model
#-------------------------------------------------------------------------------
monitor = EarlyStopping(monitor='val_loss', min_delta=1e-8, patience=5, verbose=0, mode='auto', restore_best_weights=True)
history = model.fit(X_train_std, y_train_std, validation_data=(X_test_std, y_test_std), batch_size=32, epochs=30, callbacks=[monitor])

# test of accuracy of predictions of energy y on 10 samples
i = np.random.randint(0, X_test_std.shape[0]-10)
x_10 = X_test_std[i:i+10]
y_10 = y_test_std[i:i+10]
print(x_10.shape)
pred = model.predict(x_10)
print(pred.dtype)
print(y.dtype)


for i in range(10):
  print('actual = {} | prediction = {}'.format(y_10[i], pred[i]))

#--------------------------- MODEL SAVER/LOADER FUNCTIONS ------------------------------
def weights_saver(model, download=None):
  now = datetime.now()
  time_date = now.strftime("(%H%M_%d%m)")
  model_name = '{}_{}'.format(str(model.name[0]),time_date)
  model.save_weights(model_name, save_format='h5')
  if download is not None:
    from google.colab import files
    files.download(model_name)

def model_loader(weights_file_name, model_type=None):
  model_name = str(model_type) + '_model'
  build_model = eval(model_name)
  model = build_model()
  model.load_weights(weights_file_name)
  return model

save, dl = 0, None
if save == 1:
  weights_saver(G, download=dl)
  weights_saver(D, download=dl)

